import base64
import hashlib
import json
import re
from datetime import datetime
from typing import Any, Dict, List

from langchain_core.tools import tool

from myth_config import load_dotenv
from tools.utilities.report import format_industrial_result

load_dotenv()

# ==============================================================================
# ðŸ› ï¸ Core Utilities
# ==============================================================================


@tool
def hash_generator(text: str, algorithm: str = "sha256") -> str:
    """
    Generate cryptographic hashes for text/data (utility tool).
    Useful for: Generating checksums, password hashing, data integrity verification.
    Use when: User wants to hash text, generate checksum, or create hash values.
    Supported: md5, sha1, sha256, sha512
    """
    try:
        algorithm = algorithm.lower()
        supported = ["md5", "sha1", "sha256", "sha512"]

        if algorithm not in supported:
            return f"Unsupported algorithm. Choose from: {', '.join(supported)}"

        if algorithm == "md5":
            hash_obj = hashlib.md5(text.encode())
        elif algorithm == "sha1":
            hash_obj = hashlib.sha1(text.encode())
        elif algorithm == "sha256":
            hash_obj = hashlib.sha256(text.encode())
        elif algorithm == "sha512":
            hash_obj = hashlib.sha512(text.encode())

        result = {
            "input_snippet": text[:50] + ("..." if len(text) > 50 else ""),
            "algorithm": algorithm,
            "hash": hash_obj.hexdigest(),
            "timestamp": datetime.now().isoformat(),
        }

        return json.dumps(result, indent=2)
    except Exception as e:
        return f"Hash generation error: {str(e)}"


@tool
def base64_encoder(text: str) -> str:
    """
    Encode or decode Base64 strings (utility tool).
    Useful for: Analyzing encoded strings, data obfuscation, or format conversion.
    Automatically detects if input is likely Base64 and decodes it; otherwise encodes it.
    """
    try:
        # Check if it's likely base64
        base64_pattern = re.compile(r"^[A-Za-z0-9+/]*={0,2}$")
        is_base64 = (
            len(text.strip()) > 0
            and len(text.strip()) % 4 == 0
            and base64_pattern.match(text.strip())
        )

        if is_base64:
            try:
                decoded = base64.b64decode(text.strip()).decode(
                    "utf-8", errors="ignore"
                )
                return json.dumps(
                    {
                        "operation": "decode",
                        "input": text.strip(),
                        "output": decoded,
                        "note": "Automatic detection: Decoded from Base64",
                    },
                    indent=2,
                )
            except Exception:
                pass  # Fall through to encoding if decoding fails

        # Encode
        encoded = base64.b64encode(text.encode()).decode("utf-8")
        return json.dumps(
            {
                "operation": "encode",
                "input": text[:100] + ("..." if len(text) > 100 else ""),
                "output": encoded,
            },
            indent=2,
        )
    except Exception as e:
        return f"Base64 operation error: {str(e)}"


@tool
def entropy_analyzer(text: str) -> str:
    """
    Calculates the Shannon entropy of a string to detect potential secrets, keys, or passwords.
    Industry-grade for high-fidelity secret discovery and data leakage prevention.
    """
    import math
    from collections import Counter

    try:
        if not text:
            return format_industrial_result(
                "entropy_analyzer", "Error", error="Empty input"
            )

        # Calculate Shannon Entropy
        counts = Counter(text)
        total = len(text)
        entropy = -sum(
            (count / total) * math.log2(count / total) for count in counts.values()
        )

        # Heuristic for potential secrets (entropy > 4.5 is high for typical text)
        is_suspicious = entropy > 4.5

        return format_industrial_result(
            "entropy_analyzer",
            "Analysis Complete",
            confidence=0.9,
            impact="HIGH" if is_suspicious else "LOW",
            raw_data={
                "entropy": round(entropy, 2),
                "is_suspicious": is_suspicious,
                "length": total,
            },
            summary=f"Entropy analysis for input finished. Shannon Entropy: {round(entropy, 2)}. {'SUSPICIOUS: Potential high-entropy secret detected.' if is_suspicious else 'Input appears to be low-entropy text.'}",
        )
    except Exception as e:
        return format_industrial_result("entropy_analyzer", "Error", error=str(e))


@tool
def universal_data_transformer(
    input_data: str, input_format: str, output_format: str
) -> str:
    """
    Sovereign-grade conversion between data formats (JSON, YAML, XML, TOML).
    Industry-grade for ensuring data interoperability and configuration management.
    """
    try:
        # Integrated support for XML and TOML

        import yaml

        try:
            import toml
        except ImportError:
            toml = None

        # Step 1: Parse input
        data = {}
        if input_format.lower() == "json":
            data = json.loads(input_data)
        elif input_format.lower() == "yaml":
            data = yaml.safe_load(input_data)
        elif input_format.lower() == "xml":
            import xmltodict

            data = xmltodict.parse(input_data)
        elif input_format.lower() == "toml" and toml:
            data = toml.loads(input_data)
        else:
            return format_industrial_result(
                "universal_data_transformer",
                "Error",
                error=f"Unsupported or uninstalled input format: {input_format}",
            )

        # Step 2: Convert to output
        output_data = ""
        if output_format.lower() == "json":
            output_data = json.dumps(data, indent=4)
        elif output_format.lower() == "yaml":
            output_data = yaml.dump(data, sort_keys=False)
        elif output_format.lower() == "xml":
            import xmltodict

            output_data = xmltodict.unparse(data, pretty=True)
        elif output_format.lower() == "toml" and toml:
            output_data = toml.dumps(data)
        else:
            return format_industrial_result(
                "universal_data_transformer",
                "Error",
                error=f"Unsupported or uninstalled output format: {output_format}",
            )

        return format_industrial_result(
            "universal_data_transformer",
            "Transformation Success",
            confidence=1.0,
            impact="LOW",
            raw_data={
                "input_format": input_format,
                "output_format": output_format,
                "output": output_data,
            },
            summary=f"Sovereign-grade data transformation from {input_format} to {output_format} completed successfully.",
        )
    except Exception as e:
        return format_industrial_result(
            "universal_data_transformer", "Error", error=str(e)
        )


@tool
def apex_pattern_matcher(text: str, regex_list: List[str]) -> str:
    """
    High-performance multi-regex matching engine for large datasets.
    Industry-grade for high-fidelity pattern discovery and automated data extraction.
    """
    try:
        import re

        results = []
        for pattern in regex_list:
            try:
                matches = re.findall(pattern, text)
                if matches:
                    results.append(
                        {
                            "pattern": pattern,
                            "count": len(matches),
                            "first_match": str(matches[0]),
                        }
                    )
            except Exception:
                continue

        return format_industrial_result(
            "apex_pattern_matcher",
            "Pattern Matching Complete",
            confidence=1.0,
            impact="LOW",
            raw_data={"matches": results},
            summary=f"Apex pattern matcher finished. Scanned text for {len(regex_list)} patterns and found {len(results)} matches.",
        )
    except Exception as e:
        return format_industrial_result("apex_pattern_matcher", "Error", error=str(e))


@tool
def resonance_schema_validator(
    data: Dict[str, Any], schema_type: str = "UTILITY_RESULT"
) -> str:
    """
    Enforces strict industrial-grade schemas (JSON/YAML) on data transformers and analyzers.
    Industry-grade for ensuring absolute data integrity and multi-node resonance.
    """
    try:
        # Technical schema enforcement using strict field validation
        # Industrial-grade for absolute data integrity.
        schema_templates = {
            "UTILITY_RESULT": ["tool", "status", "confidence", "summary", "raw_data"],
            "MISSION_INTEL": ["target", "findings", "timestamp", "impact_score"],
        }

        required_fields = schema_templates.get(
            schema_type, schema_templates["UTILITY_RESULT"]
        )
        missing = [f for f in required_fields if f not in data]

        if missing:
            return format_industrial_result(
                "resonance_schema_validator",
                "Validation Failed",
                confidence=1.0,
                impact="LOW",
                error=f"Critical data integrity failure: session data missing required fields {missing}",
            )

        return format_industrial_result(
            "resonance_schema_validator",
            "Validation Success",
            confidence=1.0,
            impact="LOW",
            raw_data={
                "schema": schema_type,
                "valid": True,
                "integrity_hash": hashlib.sha256(str(data).encode()).hexdigest(),
            },
            summary=f"Data successfully validated against industrial-grade {schema_type} resonance schema with hash stability.",
        )
    except Exception as e:
        return format_industrial_result(
            "resonance_schema_validator", "Error", error=str(e)
        )


@tool
def quantum_data_compressor(data: str, action: str = "compress") -> str:
    """
    Ultra-high ratio, hash-stable data compression for long-term archival of mission-critical telemetry.
    Industry-grade for ensuring absolute data immortality and storage efficiency.
    """
    try:
        import base64
        import zlib

        if action == "compress":
            compressed = zlib.compress(data.encode())
            encoded = base64.b64encode(compressed).decode()
            return format_industrial_result(
                "quantum_data_compressor",
                "Compression Success",
                confidence=1.0,
                impact="LOW",
                raw_data={
                    "original_size": len(data),
                    "compressed_size": len(encoded),
                    "ratio": f"{round(len(encoded) / len(data) * 100, 2)}%",
                },
                summary=f"Quantum data compression finished. Reduced telemetry size by {round((1 - len(encoded) / len(data)) * 100, 2)}%.",
            )
        else:
            decoded = base64.b64decode(data)
            decompressed = zlib.decompress(decoded).decode()
            return format_industrial_result(
                "quantum_data_compressor",
                "Decompression Success",
                confidence=1.0,
                impact="LOW",
                raw_data={"decompressed_size": len(decompressed)},
                summary="Quantum data decompression finished with 100% hash stability.",
            )
    except Exception as e:
        return format_industrial_result(
            "quantum_data_compressor", "Error", error=str(e)
        )
